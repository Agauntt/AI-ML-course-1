{"cells":[{"cell_type":"markdown","metadata":{"id":"37puETfgRzzg"},"source":["# Data Preprocessing Tools"]},{"cell_type":"markdown","metadata":{"id":"EoRP98MpR-qj"},"source":["## Importing the libraries"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.impute import SimpleImputer"]},{"cell_type":"markdown","metadata":{"id":"RopL7tUZSQkT"},"source":["## Importing the dataset"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["dataset = pd.read_csv('Data.csv')\n","\n","x = dataset.iloc[:, :-1].values\n","y = dataset.iloc[:, -1].values"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[['France' 44.0 72000.0]\n"," ['Spain' 27.0 48000.0]\n"," ['Germany' 30.0 54000.0]\n"," ['Spain' 38.0 61000.0]\n"," ['Germany' 40.0 nan]\n"," ['France' 35.0 58000.0]\n"," ['Spain' nan 52000.0]\n"," ['France' 48.0 79000.0]\n"," ['Germany' 50.0 83000.0]\n"," ['France' 37.0 67000.0]]\n"]}],"source":["print(x)"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"]}],"source":["print(y)"]},{"cell_type":"markdown","metadata":{},"source":["## Check for missing data"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["   Country   Age   Salary Purchased\n","4  Germany  40.0      NaN       Yes\n","6    Spain   NaN  52000.0        No\n","['Age', 'Salary']\n","[1, 2]\n"]}],"source":["has_nan_rows = dataset[dataset.isna().any(axis=1)]\n","has_nan_columns = dataset.columns[dataset.isna().any(axis=0)].tolist()\n","column_indexes = []\n","for i in has_nan_columns:\n","    index = dataset.columns.get_loc(i)\n","    column_indexes.append(index)\n","print(has_nan_rows)\n","print(has_nan_columns)\n","print(column_indexes)"]},{"cell_type":"markdown","metadata":{"id":"nhfKXNxlSabC"},"source":["## Taking care of missing data"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[['France' 44.0 72000.0]\n"," ['Spain' 27.0 48000.0]\n"," ['Germany' 30.0 54000.0]\n"," ['Spain' 38.0 61000.0]\n"," ['Germany' 40.0 63777.77777777778]\n"," ['France' 35.0 58000.0]\n"," ['Spain' 38.77777777777778 52000.0]\n"," ['France' 48.0 79000.0]\n"," ['Germany' 50.0 83000.0]\n"," ['France' 37.0 67000.0]]\n"]}],"source":["imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n","imputer.fit(x[:, column_indexes])\n","x[:, column_indexes] = imputer.transform(x[:, column_indexes])\n","\n","print(x)\n"]},{"cell_type":"markdown","metadata":{"id":"CriG6VzVSjcK"},"source":["## Encoding categorical data"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","\n","ct = ColumnTransformer(\n","    transformers=[('encoder',\n","                   OneHotEncoder(),\n","                   [0])],\n","    remainder='passthrough'\n",")\n","X = np.array(ct.fit_transform(x))"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1.0 0.0 0.0 44.0 72000.0]\n"," [0.0 0.0 1.0 27.0 48000.0]\n"," [0.0 1.0 0.0 30.0 54000.0]\n"," [0.0 0.0 1.0 38.0 61000.0]\n"," [0.0 1.0 0.0 40.0 63777.77777777778]\n"," [1.0 0.0 0.0 35.0 58000.0]\n"," [0.0 0.0 1.0 38.77777777777778 52000.0]\n"," [1.0 0.0 0.0 48.0 79000.0]\n"," [0.0 1.0 0.0 50.0 83000.0]\n"," [1.0 0.0 0.0 37.0 67000.0]]\n"]}],"source":["print(X)"]},{"cell_type":"markdown","metadata":{"id":"AhSpdQWeSsFh"},"source":["### Encoding the Independent Variable"]},{"cell_type":"markdown","metadata":{"id":"DXh8oVSITIc6"},"source":["### Encoding the Dependent Variable"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","y = le.fit_transform(y)"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0 1 0 0 1 1 0 1 0 1]\n"]}],"source":["print(y)"]},{"cell_type":"markdown","metadata":{"id":"qb_vcgm3qZKW"},"source":["## Splitting the dataset into the Training set and Test set"]},{"cell_type":"markdown","metadata":{"id":"TpGqbS4TqkIR"},"source":["## Feature Scaling"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOihYlX/ooG5h+qw0sLIjn8","collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"MachineLearningUdemy","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}
